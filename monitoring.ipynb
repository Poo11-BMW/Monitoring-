{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c59e929c-fe10-4d5e-8661-66b6f312f2e2",
   "metadata": {},
   "source": [
    "S6 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "29c16168-5c11-4f50-be87-ff1216f463f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized dataset with 56 videos.\n",
      "Initialized dataset with 12 videos.\n",
      "Initialized dataset with 12 videos.\n",
      "Epoch 1, Training Accuracy: 0.7679\n",
      "Epoch 1, Validation Accuracy: 1.0000\n",
      "Epoch 2, Training Accuracy: 0.9464\n",
      "Epoch 2, Validation Accuracy: 0.9167\n",
      "Epoch 3, Training Accuracy: 0.9643\n",
      "Epoch 3, Validation Accuracy: 1.0000\n",
      "Epoch 4, Training Accuracy: 0.9286\n",
      "Epoch 4, Validation Accuracy: 1.0000\n",
      "Epoch 5, Training Accuracy: 0.9107\n",
      "Epoch 5, Validation Accuracy: 0.9167\n",
      "Epoch 6, Training Accuracy: 1.0000\n",
      "Epoch 6, Validation Accuracy: 1.0000\n",
      "Epoch 7, Training Accuracy: 0.9107\n",
      "Epoch 7, Validation Accuracy: 0.7500\n",
      "Epoch 8, Training Accuracy: 0.9464\n",
      "Epoch 8, Validation Accuracy: 0.9167\n",
      "Epoch 9, Training Accuracy: 1.0000\n",
      "Epoch 9, Validation Accuracy: 1.0000\n",
      "Epoch 10, Training Accuracy: 0.9464\n",
      "Epoch 10, Validation Accuracy: 0.9167\n",
      "\n",
      "ðŸš€ Overall Model Accuracy: 94.17%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "import torchmetrics\n",
    "import timm\n",
    "import numpy as np\n",
    "\n",
    "# Data preparation\n",
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, video_files, root_dir, transform=None):\n",
    "        self.video_files = video_files\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform or transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize((300, 300)),  # EfficientNet S6 input size\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomRotation(10),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        print(f\"Initialized dataset with {len(video_files)} videos.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_file = self.video_files[idx]\n",
    "        video_path = os.path.join(self.root_dir, video_file)\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frames = []\n",
    "\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Convert OpenCV BGR to RGB\n",
    "            frame = self.transform(frame)  # Apply transformations (ensures shape: [3, 300, 300])\n",
    "            frames.append(frame)\n",
    "\n",
    "        cap.release()\n",
    "\n",
    "        # ðŸ›  Fix: Ensure at least one frame is present\n",
    "        if len(frames) == 0:\n",
    "            return torch.zeros(3, 300, 300), torch.tensor(-1, dtype=torch.float32)  # Default tensor for empty videos\n",
    "\n",
    "        # ðŸ›  Fix: Stack frames correctly (ensures consistent tensor shape)\n",
    "        frames = torch.stack(frames)  # Shape: [num_frames, 3, 300, 300]\n",
    "        frames = torch.mean(frames, dim=0)  # Take mean across time axis â†’ [3, 300, 300]\n",
    "\n",
    "        # ðŸ›  Fix: Convert label to `float32`\n",
    "        label = torch.tensor(0.0 if 'Makeup' in video_file else 1.0, dtype=torch.float32)\n",
    "\n",
    "        return frames, label\n",
    "\n",
    "# Defining the model using EfficientNet S6\n",
    "class EfficientNetS6Model(nn.Module):\n",
    "    def __init__(self, num_classes=1):\n",
    "        super(EfficientNetS6Model, self).__init__()\n",
    "        self.base_model = timm.create_model('efficientnetv2_rw_s', pretrained=True)\n",
    "        num_features = self.base_model.classifier.in_features\n",
    "        self.base_model.classifier = nn.Linear(num_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.base_model(x)  # No sigmoid here; apply it in loss calculation\n",
    "\n",
    "# datasets and dataloaders\n",
    "def setup_datasets(root_dir):\n",
    "    video_files = os.listdir(root_dir)\n",
    "    np.random.shuffle(video_files)\n",
    "    num_videos = len(video_files)\n",
    "    train_split = int(0.7 * num_videos)\n",
    "    val_split = int(0.85 * num_videos)\n",
    "\n",
    "    train_files = video_files[:train_split]\n",
    "    val_files = video_files[train_split:val_split]\n",
    "    test_files = video_files[val_split:]\n",
    "\n",
    "    train_dataset = VideoDataset(train_files, root_dir)\n",
    "    val_dataset = VideoDataset(val_files, root_dir)\n",
    "    test_dataset = VideoDataset(test_files, root_dir)\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "root_dir = './Assessment_Dataset'\n",
    "train_dataset, val_dataset, test_dataset = setup_datasets(root_dir)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = EfficientNetS6Model().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "accuracy_metric = torchmetrics.Accuracy(task=\"binary\").to(device)\n",
    "total_correct = 0\n",
    "total_samples = 0\n",
    "\n",
    "# Training \n",
    "def train_and_validate(model, train_loader, val_loader, criterion, optimizer, num_epochs=10):\n",
    "    global total_correct, total_samples  \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for frames, labels in train_loader:\n",
    "            if torch.any(labels == -1):  \n",
    "                continue\n",
    "            frames = frames.to(device)\n",
    "            labels = labels.unsqueeze(1).to(device)  \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(frames)  \n",
    "            loss = criterion(outputs, labels)  \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            \n",
    "            preds = torch.sigmoid(outputs)  \n",
    "            accuracy_metric.update(preds, labels.int())\n",
    "\n",
    "        train_accuracy = accuracy_metric.compute()\n",
    "        print(f'Epoch {epoch+1}, Training Accuracy: {train_accuracy:.4f}')\n",
    "        accuracy_metric.reset()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        for frames, labels in val_loader:\n",
    "            if torch.any(labels == -1):  \n",
    "                continue\n",
    "            frames = frames.to(device)\n",
    "            labels = labels.unsqueeze(1).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(frames)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                # Compute accuracy\n",
    "                preds = torch.sigmoid(outputs)\n",
    "                accuracy_metric.update(preds, labels.int())\n",
    "\n",
    "                # ðŸ›  NEW: Track overall correct predictions\n",
    "                total_correct += (preds.round() == labels).sum().item()\n",
    "                total_samples += labels.size(0)\n",
    "\n",
    "        val_accuracy = accuracy_metric.compute()\n",
    "        print(f'Epoch {epoch+1}, Validation Accuracy: {val_accuracy:.4f}')\n",
    "        accuracy_metric.reset()\n",
    "\n",
    "    \n",
    "    overall_accuracy = (total_correct / total_samples) * 100 if total_samples > 0 else 0\n",
    "    print(f\"\\nðŸš€ Overall Model Accuracy: {overall_accuracy:.2f}%\")\n",
    "\n",
    "train_and_validate(model, train_loader, val_loader, criterion, optimizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f4045a-72c9-4324-babb-0faec1a6a238",
   "metadata": {},
   "source": [
    "Video Mamba Suit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "457cd607-5b27-483a-8b8a-b3a5840d9675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized dataset with 56 videos.\n",
      "Initialized dataset with 12 videos.\n",
      "Initialized dataset with 12 videos.\n",
      "Epoch 1, Training Accuracy: 0.5577\n",
      "Epoch 1, Validation Accuracy: 0.6667\n",
      "Epoch 2, Training Accuracy: 0.8654\n",
      "Epoch 2, Validation Accuracy: 0.8333\n",
      "Epoch 3, Training Accuracy: 0.9423\n",
      "Epoch 3, Validation Accuracy: 0.6667\n",
      "Epoch 4, Training Accuracy: 1.0000\n",
      "Epoch 4, Validation Accuracy: 0.7500\n",
      "Epoch 5, Training Accuracy: 0.9423\n",
      "Epoch 5, Validation Accuracy: 0.7500\n",
      "Epoch 6, Training Accuracy: 0.9231\n",
      "Epoch 6, Validation Accuracy: 0.6667\n",
      "Epoch 7, Training Accuracy: 0.9038\n",
      "Epoch 7, Validation Accuracy: 0.5000\n",
      "Epoch 8, Training Accuracy: 0.9231\n",
      "Epoch 8, Validation Accuracy: 0.8333\n",
      "Epoch 9, Training Accuracy: 0.9231\n",
      "Epoch 9, Validation Accuracy: 0.5833\n",
      "Epoch 10, Training Accuracy: 0.8654\n",
      "Epoch 10, Validation Accuracy: 0.7500\n",
      "\n",
      "ðŸš€ Overall Model Accuracy: 85.00%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "import torchmetrics\n",
    "import numpy as np\n",
    "\n",
    "# Video Mamba Block\n",
    "class VideoMambaBlock(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(VideoMambaBlock, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "# Video Mamba Model\n",
    "class VideoMamba(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers=3):\n",
    "        super(VideoMamba, self).__init__()\n",
    "        self.layers = nn.ModuleList([VideoMambaBlock(input_dim if i == 0 else hidden_dim, hidden_dim) for i in range(num_layers)])\n",
    "        self.output_layer = nn.Linear(hidden_dim, hidden_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return self.output_layer(x)\n",
    "\n",
    "# Data Preparation\n",
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, video_files, root_dir, transform=None):\n",
    "        self.video_files = video_files\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform or transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomRotation(10),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        print(f\"Initialized dataset with {len(video_files)} videos.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_file = self.video_files[idx]\n",
    "        video_path = os.path.join(self.root_dir, video_file)\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frames = []\n",
    "\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frame = self.transform(frame)\n",
    "            frames.append(frame)\n",
    "\n",
    "        cap.release()\n",
    "\n",
    "        if len(frames) == 0:\n",
    "            return torch.zeros(3, 224, 224), torch.tensor(-1, dtype=torch.float32)\n",
    "\n",
    "        frames = torch.stack(frames)\n",
    "        frames = torch.mean(frames, dim=0)\n",
    "        label = torch.tensor(0.0 if 'Makeup' in video_file else 1.0, dtype=torch.float32)\n",
    "        return frames, label\n",
    "\n",
    "# Video Mamba model for Temporal Action Localization\n",
    "class VideoMambaTALModel(nn.Module):\n",
    "    def __init__(self, num_classes=1, hidden_dim=256, num_layers=3):\n",
    "        super(VideoMambaTALModel, self).__init__()\n",
    "        self.mamba_encoder = VideoMamba(input_dim=3*224*224, hidden_dim=hidden_dim, num_layers=num_layers)\n",
    "        self.classifier = nn.Linear(hidden_dim, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.mamba_encoder(x)\n",
    "        return self.classifier(x)\n",
    "\n",
    "# datasets and dataloaders\n",
    "def setup_datasets(root_dir):\n",
    "    video_files = os.listdir(root_dir)\n",
    "    np.random.shuffle(video_files)\n",
    "    num_videos = len(video_files)\n",
    "    train_split = int(0.7 * num_videos)\n",
    "    val_split = int(0.85 * num_videos)\n",
    "\n",
    "    train_files = video_files[:train_split]\n",
    "    val_files = video_files[train_split:val_split]\n",
    "    test_files = video_files[val_split:]\n",
    "\n",
    "    train_dataset = VideoDataset(train_files, root_dir)\n",
    "    val_dataset = VideoDataset(val_files, root_dir)\n",
    "    test_dataset = VideoDataset(test_files, root_dir)\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "root_dir = './Assessment_Dataset'\n",
    "train_dataset, val_dataset, test_dataset = setup_datasets(root_dir)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = VideoMambaTALModel().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "accuracy_metric = torchmetrics.Accuracy(task=\"binary\").to(device)\n",
    "\n",
    "#training\n",
    "def train_and_validate(model, train_loader, val_loader, criterion, optimizer, num_epochs=10):\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for frames, labels in train_loader:\n",
    "            if torch.any(labels == -1):\n",
    "                continue\n",
    "            frames = frames.to(device)\n",
    "            labels = labels.unsqueeze(1).to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(frames)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            preds = torch.sigmoid(outputs)\n",
    "            accuracy_metric.update(preds, labels.int())\n",
    "            total_correct += (preds.round() == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "        \n",
    "        train_accuracy = accuracy_metric.compute()\n",
    "        print(f'Epoch {epoch+1}, Training Accuracy: {train_accuracy:.4f}')\n",
    "        accuracy_metric.reset()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        for frames, labels in val_loader:\n",
    "            if torch.any(labels == -1):\n",
    "                continue\n",
    "            frames = frames.to(device)\n",
    "            labels = labels.unsqueeze(1).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(frames)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                preds = torch.sigmoid(outputs)\n",
    "                accuracy_metric.update(preds, labels.int())\n",
    "                total_correct += (preds.round() == labels).sum().item()\n",
    "                total_samples += labels.size(0)\n",
    "\n",
    "        val_accuracy = accuracy_metric.compute()\n",
    "        print(f'Epoch {epoch+1}, Validation Accuracy: {val_accuracy:.4f}')\n",
    "        accuracy_metric.reset()\n",
    "    \n",
    "    overall_accuracy = (total_correct / total_samples) * 100 if total_samples > 0 else 0\n",
    "    print(f'\\nðŸš€ Overall Model Accuracy: {overall_accuracy:.2f}%')\n",
    "\n",
    "train_and_validate(model, train_loader, val_loader, criterion, optimizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae722f7-1404-42e6-8510-872e3be3b9f4",
   "metadata": {},
   "source": [
    "PRN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "febe5663-85bd-461b-8bf5-f1bdd9af6ff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized dataset with 56 videos.\n",
      "Initialized dataset with 12 videos.\n",
      "Initialized dataset with 12 videos.\n",
      "Epoch 1, Training Accuracy: 0.6538\n",
      "Epoch 1, Validation Accuracy: 0.5000\n",
      "Epoch 2, Training Accuracy: 0.7115\n",
      "Epoch 2, Validation Accuracy: 0.5000\n",
      "Epoch 3, Training Accuracy: 0.9038\n",
      "Epoch 3, Validation Accuracy: 0.5000\n",
      "Epoch 4, Training Accuracy: 0.9231\n",
      "Epoch 4, Validation Accuracy: 0.6667\n",
      "Epoch 5, Training Accuracy: 0.8654\n",
      "Epoch 5, Validation Accuracy: 0.5000\n",
      "Epoch 6, Training Accuracy: 0.8846\n",
      "Epoch 6, Validation Accuracy: 0.5000\n",
      "Epoch 7, Training Accuracy: 0.9615\n",
      "Epoch 7, Validation Accuracy: 1.0000\n",
      "Epoch 8, Training Accuracy: 1.0000\n",
      "Epoch 8, Validation Accuracy: 1.0000\n",
      "Epoch 9, Training Accuracy: 1.0000\n",
      "Epoch 9, Validation Accuracy: 1.0000\n",
      "Epoch 10, Training Accuracy: 1.0000\n",
      "Epoch 10, Validation Accuracy: 1.0000\n",
      "\n",
      "ðŸš€ Overall Model Accuracy: 85.78%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "import torchmetrics\n",
    "import numpy as np\n",
    "\n",
    "# PRN Block\n",
    "class PRNBlock(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super(PRNBlock, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "# PRN Model\n",
    "class PRN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers=3):\n",
    "        super(PRN, self).__init__()\n",
    "        self.layers = nn.ModuleList([PRNBlock(input_dim if i == 0 else hidden_dim, hidden_dim) for i in range(num_layers)])\n",
    "        self.output_layer = nn.Linear(hidden_dim, hidden_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return self.output_layer(x)\n",
    "\n",
    "# Data Preparation\n",
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, video_files, root_dir, transform=None):\n",
    "        self.video_files = video_files\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform or transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomRotation(10),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        print(f\"Initialized dataset with {len(video_files)} videos.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_file = self.video_files[idx]\n",
    "        video_path = os.path.join(self.root_dir, video_file)\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frames = []\n",
    "\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frame = self.transform(frame)\n",
    "            frames.append(frame)\n",
    "\n",
    "        cap.release()\n",
    "\n",
    "        if len(frames) == 0:\n",
    "            return torch.zeros(3, 224, 224), torch.tensor(-1, dtype=torch.float32)\n",
    "\n",
    "        frames = torch.stack(frames)\n",
    "        frames = torch.mean(frames, dim=0)\n",
    "        label = torch.tensor(0.0 if 'Makeup' in video_file else 1.0, dtype=torch.float32)\n",
    "        return frames, label\n",
    "\n",
    "# PRN model for Temporal Action Localization\n",
    "class PRNTALModel(nn.Module):\n",
    "    def __init__(self, num_classes=1, hidden_dim=256, num_layers=3):\n",
    "        super(PRNTALModel, self).__init__()\n",
    "        self.prn_encoder = PRN(input_dim=3*224*224, hidden_dim=hidden_dim, num_layers=num_layers)\n",
    "        self.classifier = nn.Linear(hidden_dim, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.prn_encoder(x)\n",
    "        return self.classifier(x)\n",
    "\n",
    "# datasets and dataloaders\n",
    "def setup_datasets(root_dir):\n",
    "    video_files = os.listdir(root_dir)\n",
    "    np.random.shuffle(video_files)\n",
    "    num_videos = len(video_files)\n",
    "    train_split = int(0.7 * num_videos)\n",
    "    val_split = int(0.85 * num_videos)\n",
    "\n",
    "    train_files = video_files[:train_split]\n",
    "    val_files = video_files[train_split:val_split]\n",
    "    test_files = video_files[val_split:]\n",
    "\n",
    "    train_dataset = VideoDataset(train_files, root_dir)\n",
    "    val_dataset = VideoDataset(val_files, root_dir)\n",
    "    test_dataset = VideoDataset(test_files, root_dir)\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "root_dir = './Assessment_Dataset'\n",
    "train_dataset, val_dataset, test_dataset = setup_datasets(root_dir)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = PRNTALModel().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "accuracy_metric = torchmetrics.Accuracy(task=\"binary\").to(device)\n",
    "\n",
    "#training\n",
    "def train_and_validate(model, train_loader, val_loader, criterion, optimizer, num_epochs=10):\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for frames, labels in train_loader:\n",
    "            if torch.any(labels == -1):\n",
    "                continue\n",
    "            frames = frames.to(device)\n",
    "            labels = labels.unsqueeze(1).to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            outputs = model(frames)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            preds = torch.sigmoid(outputs)\n",
    "            accuracy_metric.update(preds, labels.int())\n",
    "            total_correct += (preds.round() == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "        \n",
    "        train_accuracy = accuracy_metric.compute()\n",
    "        print(f'Epoch {epoch+1}, Training Accuracy: {train_accuracy:.4f}')\n",
    "        accuracy_metric.reset()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        for frames, labels in val_loader:\n",
    "            if torch.any(labels == -1):\n",
    "                continue\n",
    "            frames = frames.to(device)\n",
    "            labels = labels.unsqueeze(1).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(frames)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "                preds = torch.sigmoid(outputs)\n",
    "                accuracy_metric.update(preds, labels.int())\n",
    "                total_correct += (preds.round() == labels).sum().item()\n",
    "                total_samples += labels.size(0)\n",
    "\n",
    "        val_accuracy = accuracy_metric.compute()\n",
    "        print(f'Epoch {epoch+1}, Validation Accuracy: {val_accuracy:.4f}')\n",
    "        accuracy_metric.reset()\n",
    "    \n",
    "    overall_accuracy = (total_correct / total_samples) * 100 if total_samples > 0 else 0\n",
    "    print(f'\\nðŸš€ Overall Model Accuracy: {overall_accuracy:.2f}%')\n",
    "\n",
    "train_and_validate(model, train_loader, val_loader, criterion, optimizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4317d0-225c-4f28-bf45-e0912221b128",
   "metadata": {},
   "source": [
    "Proposed Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e935c464-8b8a-49e6-992e-539fa3e536ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized dataset with 56 videos.\n",
      "Initialized dataset with 12 videos.\n",
      "Initialized dataset with 12 videos.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91636\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\91636\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Training Accuracy: 0.7273\n",
      "Epoch 1, Validation Accuracy: 0.9167\n",
      "Epoch 2, Training Accuracy: 0.8364\n",
      "Epoch 2, Validation Accuracy: 0.9167\n",
      "Epoch 3, Training Accuracy: 0.8727\n",
      "Epoch 3, Validation Accuracy: 0.7500\n",
      "Epoch 4, Training Accuracy: 0.9455\n",
      "Epoch 4, Validation Accuracy: 1.0000\n",
      "Epoch 5, Training Accuracy: 0.9273\n",
      "Epoch 5, Validation Accuracy: 0.6667\n",
      "Epoch 6, Training Accuracy: 0.8364\n",
      "Epoch 6, Validation Accuracy: 0.9167\n",
      "Epoch 7, Training Accuracy: 0.8727\n",
      "Epoch 7, Validation Accuracy: 1.0000\n",
      "Epoch 8, Training Accuracy: 0.9091\n",
      "Epoch 8, Validation Accuracy: 1.0000\n",
      "Epoch 9, Training Accuracy: 0.9455\n",
      "Epoch 9, Validation Accuracy: 1.0000\n",
      "Epoch 10, Training Accuracy: 0.9636\n",
      "Epoch 10, Validation Accuracy: 1.0000\n",
      "\n",
      "ðŸš€ Overall Model Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "import torchmetrics\n",
    "import numpy as np\n",
    "\n",
    "# Data Preparation\n",
    "class VideoDataset(Dataset):\n",
    "    def __init__(self, video_files, root_dir, transform=None):\n",
    "        self.video_files = video_files\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform or transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.RandomHorizontalFlip(),  # Data augmentation\n",
    "            transforms.RandomRotation(10),      # Random rotation\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        print(f\"Initialized dataset with {len(video_files)} videos.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.video_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        video_file = self.video_files[idx]\n",
    "        video_path = os.path.join(self.root_dir, video_file)\n",
    "        cap = cv2.VideoCapture(video_path)\n",
    "        frames = []\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frame = self.transform(frame)\n",
    "            frames.append(frame)\n",
    "        cap.release()\n",
    "        \n",
    "        if len(frames) == 0:\n",
    "            return None  # Return None if no frames were found\n",
    "\n",
    "        frames = torch.stack(frames)  # Stack frames into tensor\n",
    "        frames = torch.mean(frames, dim=0, keepdim=True)  # Take average across time\n",
    "        \n",
    "        label = 0 if 'Makeup' in video_file else 1  # Assign binary label\n",
    "        return frames.squeeze(0), torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "# the model\n",
    "class BabyCrawlingModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BabyCrawlingModel, self).__init__()\n",
    "        self.feature_extractor = models.mobilenet_v2(pretrained=True).features\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.temporal_model = nn.Linear(1280, 512)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),  \n",
    "            nn.Linear(512, 1)  \n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.temporal_model(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# datasets and dataloaders\n",
    "def setup_datasets(root_dir):\n",
    "    video_files = os.listdir(root_dir)\n",
    "    np.random.shuffle(video_files)\n",
    "    num_videos = len(video_files)\n",
    "    train_split = int(0.7 * num_videos)\n",
    "    val_split = int(0.85 * num_videos)\n",
    "\n",
    "    train_files = video_files[:train_split]\n",
    "    val_files = video_files[train_split:val_split]\n",
    "    test_files = video_files[val_split:]\n",
    "\n",
    "    train_dataset = VideoDataset(train_files, root_dir)\n",
    "    val_dataset = VideoDataset(val_files, root_dir)\n",
    "    test_dataset = VideoDataset(test_files, root_dir)\n",
    "    \n",
    "    # Filter out None values (invalid samples)\n",
    "    train_dataset = [x for x in train_dataset if x is not None]\n",
    "    val_dataset = [x for x in val_dataset if x is not None]\n",
    "    test_dataset = [x for x in test_dataset if x is not None]\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "root_dir = './Assessment_Dataset'\n",
    "train_dataset, val_dataset, test_dataset = setup_datasets(root_dir)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)  # Increased batch size\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BabyCrawlingModel().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "accuracy_metric = torchmetrics.Accuracy(task=\"binary\").to(device)\n",
    "\n",
    "# Training \n",
    "def train_and_validate(model, train_loader, val_loader, criterion, optimizer, num_epochs=10):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for frames, labels in train_loader:\n",
    "            if frames is None:  \n",
    "                continue\n",
    "            frames = frames.to(device)\n",
    "            labels = labels.float().unsqueeze(1).to(device) \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(frames)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            accuracy_metric.update(torch.sigmoid(outputs), labels.int())\n",
    "\n",
    "    \n",
    "        train_accuracy = accuracy_metric.compute()\n",
    "        print(f'Epoch {epoch+1}, Training Accuracy: {train_accuracy:.4f}')\n",
    "        accuracy_metric.reset()\n",
    "\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        for frames, labels in val_loader:\n",
    "            if frames is None:  \n",
    "                continue\n",
    "            frames = frames.to(device)\n",
    "            labels = labels.float().unsqueeze(1).to(device)\n",
    "            with torch.no_grad():\n",
    "                outputs = model(frames)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                \n",
    "                accuracy_metric.update(torch.sigmoid(outputs), labels.int())\n",
    "\n",
    "        \n",
    "        val_accuracy = accuracy_metric.compute()\n",
    "        print(f'Epoch {epoch+1}, Validation Accuracy: {val_accuracy:.4f}')\n",
    "        accuracy_metric.reset()\n",
    "\n",
    "train_and_validate(model, train_loader, val_loader, criterion, optimizer)\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    accuracy_metric.reset()\n",
    "    \n",
    "    total_correct = 0  \n",
    "    total_samples = 0  \n",
    "    \n",
    "    for frames, labels in test_loader:\n",
    "        if frames is None:\n",
    "            continue\n",
    "        frames = frames.to(device)\n",
    "        labels = labels.float().unsqueeze(1).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(frames)\n",
    "            preds = torch.sigmoid(outputs).round()  \n",
    "            \n",
    "            total_correct += (preds == labels).sum().item()  \n",
    "            total_samples += labels.size(0)  \n",
    "    \n",
    "    overall_accuracy = (total_correct / total_samples) * 100 if total_samples > 0 else 0\n",
    "    print(f'\\nðŸš€ Overall Model Accuracy: {overall_accuracy:.2f}%')\n",
    "\n",
    "evaluate_model(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff3b1dd-4b4c-4e40-8e58-26e0574b6cbf",
   "metadata": {},
   "source": [
    "Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c34e10-e009-45b3-b535-3353ce558580",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\91636\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\91636\\anaconda3\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Werkzeug appears to be used in a production deployment. Consider switching to a production web server instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "import torch.optim as optim\n",
    "import torchmetrics\n",
    "import numpy as np\n",
    "import threading\n",
    "import time\n",
    "import queue\n",
    "from flask import Flask, Response, render_template\n",
    "from flask_socketio import SocketIO\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import io\n",
    "import base64\n",
    "\n",
    "# Define the model\n",
    "class BabyCrawlingModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BabyCrawlingModel, self).__init__()\n",
    "        self.feature_extractor = models.mobilenet_v2(pretrained=True).features\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.temporal_model = nn.Linear(1280, 512)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        x = self.adaptive_pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.temporal_model(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Initialize Flask app\n",
    "app = Flask(__name__)\n",
    "socketio = SocketIO(app)\n",
    "\n",
    "# Video source\n",
    "VIDEO_SOURCE = 0  # Change to video file path if needed\n",
    "frame_queue = queue.Queue()\n",
    "activity_data = {\"frame_count\": [], \"detections\": []}\n",
    "frame_counter = 0\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BabyCrawlingModel().to(device)\n",
    "model.eval()\n",
    "\n",
    "def generate_frames():\n",
    "    global frame_counter\n",
    "    cap = cv2.VideoCapture(VIDEO_SOURCE)\n",
    "    while True:\n",
    "        success, frame = cap.read()\n",
    "        if not success:\n",
    "            break\n",
    "        frame_counter += 1\n",
    "        frame = cv2.resize(frame, (224, 224))\n",
    "        frame_queue.put(frame)\n",
    "        _, buffer = cv2.imencode(\".jpg\", frame)\n",
    "        frame_bytes = buffer.tobytes()\n",
    "        yield (b'--frame\\r\\n' b'Content-Type: image/jpeg\\r\\n\\r\\n' + frame_bytes + b'\\r\\n')\n",
    "    cap.release()\n",
    "\n",
    "@app.route('/video_feed')\n",
    "def video_feed():\n",
    "    return Response(generate_frames(), mimetype='multipart/x-mixed-replace; boundary=frame')\n",
    "\n",
    "@app.route('/')\n",
    "def index():\n",
    "    return render_template(\"index.html\")\n",
    "\n",
    "def process_video():\n",
    "    global frame_counter\n",
    "    while True:\n",
    "        if not frame_queue.empty():\n",
    "            frame = frame_queue.get()\n",
    "            frame_tensor = torch.tensor(frame, dtype=torch.float32).permute(2, 0, 1).unsqueeze(0).to(device) / 255.0\n",
    "            with torch.no_grad():\n",
    "                output = model(frame_tensor)\n",
    "                prediction = torch.sigmoid(output).cpu().numpy()[0][0]\n",
    "            activity_data[\"frame_count\"].append(frame_counter)\n",
    "            activity_data[\"detections\"].append(prediction)\n",
    "            socketio.emit('update_chart', {\"frame\": frame_counter, \"prediction\": prediction, \"graph\": generate_graph()})\n",
    "            time.sleep(0.1)\n",
    "\n",
    "def generate_graph():\n",
    "    plt.clf()\n",
    "    if len(activity_data[\"frame_count\"]) > 0:\n",
    "        plt.plot(activity_data[\"frame_count\"], activity_data[\"detections\"], marker=\"o\", linestyle=\"-\")\n",
    "        plt.xlabel(\"Frame Number\")\n",
    "        plt.ylabel(\"Prediction Score\")\n",
    "        plt.title(\"Real-Time Activity Analytics\")\n",
    "        plt.ylim([0, 1])\n",
    "    buf = io.BytesIO()\n",
    "    plt.savefig(buf, format=\"png\")\n",
    "    buf.seek(0)\n",
    "    return base64.b64encode(buf.getvalue()).decode('utf-8')\n",
    "\n",
    "@app.route('/graph_feed')\n",
    "def graph_feed():\n",
    "    return f'<img src=\"data:image/png;base64,{generate_graph()}\" width=\"50%\">'\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    threading.Thread(target=process_video, daemon=True).start()\n",
    "    socketio.run(app, debug=True, use_reloader=False, port=5000, allow_unsafe_werkzeug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ef70c5-00bc-455b-baee-467c412c89a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
